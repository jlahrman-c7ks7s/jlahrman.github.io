{"name":"Jlahrman.GitHub.io","tagline":"","body":"Practical Machine Learning Class Assignment Write-up\r\n\r\nProject Overview\r\n\r\nThe goal of this project is to predict the manner in which participants in an exercise did the exercise. There are 20 observations to predict, after determining the best approach to prediction using a training data set. The variable to predict is the \"classe\" variable.\r\n\r\nStep 1: Inspect training data, remove unneeded rows and fields\r\nThe first step is to visually inspect the training and test data. The classe variable appears to have five possibilities: A, B, C, D, and E. This suggests a classification prediction algorithm.\r\n\r\nIn the training data set, records with a value of “yes” in the new_window field appear to be summary statistics for a group of data. These may not be useful for predicting individual observations. When these records are filtered out of the dataset, many other fields are either all blank or have values of all “NA”. These fields will not be useful in predicting the classe variable. Removing these fields reduces the number of columns in the training set from 160 to 60.\r\n\r\n\r\nStep 2: Analyze remaining variables for suitability as predictors\r\nWith several variables removed, the next step is the review the remaining variables for suitability as predictors. The nearZeroVar function is used to analyze the remaining variables, with particular attention paid to the percent Unique output. Selected variables that have very few unique values are shown in Table 1 . These variables will initially be left in as possible predictors, but may need to be removed:\r\n\r\nTable 1\r\n                     freqRatio percentUnique \r\nX                     1.000000  1.000000e+02    \r\nuser_name             1.102460  3.122398e-02    \r\nraw_timestamp_part_1  1.000000  4.355745e+00    \r\nraw_timestamp_part_2  1.000000  8.568901e+01    \r\ncvtd_timestamp        1.002041  1.040799e-01    \r\nnew_window            0.000000  5.203997e-03    \r\nnum_window            1.000000  4.459825e+00    \r\n\r\n\r\nStep 3: Run initial model to further analyze variables\r\nWith 60 variables remaining, an initial CART model is run on the full training data set using the rpart method in the caret package. The X variable (unlabeled in the .csv) is the sole variable used to create the tree. This variable appears to be a numbered index field. When plotted against the class variable in Figure 1, it can be seen that the class variable is sorted in order (A through E). \r\nFigure 1 – index Variable vs. classe Variable\r\n \r\nIt appears probable that the dataset was originally constructed by sorting the data by the classe field and then adding the index variable. This index variable is removed from the data as it will not hold predictive value.\r\n\r\nWhen a CART model is run without the “X” index field, the nodes are create by fields other than those listed in Table 1.\r\n\r\nStep 4: Cross-Validation Approach\r\nOne of the key outputs of the analysis is an estimate of the out-of-sample error. To accomplish this, a simple data partition is created in the training data. 90% of the data remains in the training data set. 10% is randomly selected to constitute a validation data set. The model will be trained on the training data, and used to predict the classe variable of the validation set. The accuracy of the validation set prediction will be the estimate of the out-of-sample error.\r\n\r\nNote that a k-folds cross-validation approach was considered. However, it is unclear whether the accuracy results of the cross-validation method using caret would produce a true out-of-sample estimate.\r\n\r\nStep 5: Model Results\r\nThree initial models are fitted to the training data, all using the caret package in R:\r\n1.\tA classification and regression tree (CART) model\r\n2.\tA random forest model\r\n3.\tA gbm boosting model\r\n\r\nNote that the seed was set to 1 in order to enable reproducible results.\r\n\r\nCART Model Results\r\nA CART model was fit to the training data using the rpart method in the caret package. \r\n\r\nThe in-sample accuracy using this method, reflecting the algorithm’s accuracy in predicting the classe variable of the training data set, is 49.86%. The out-of-sample accuracy, reflecting the algorithm’s accuracy in predicting the classe variable of the validation data set, is 48.68%. If the in-sample accuracy were substantially higher than the out-of-sample accuracy, the model may be affected by over-fitting. However, the similarity of the two accuracy rates provides evidence that the model is not affected by over-fitting.\r\n\r\nThe classification tree created using the rpart method on the training data is shown in Figure 2.\r\n\r\nFigure 2 – rpart Classification Tree\r\n\r\n \r\n\r\nRandom Forest Model Results\r\nA random forest model was fit to the training data using the rpart method in the caret package. \r\nHowever, this approach did not appear to resolve and was ended after six hours.\r\n\r\nAs a second option, the randomForest package was used to create a random forest model in R. This model resolved quickly. Both the in-sample and out-of-sample accuracy are 100%. The author has been unable to find an approach to plot the specific indicator variables and nodes within the randomForest package, but the overall statistics of both the training and validation set are shown in Figure 3 and Figure 4.\r\n\r\nFigure 3 – Overall Statistics of randomForest Algorithm on Training Data\r\n\r\nOverall Statistics\r\n                                     \r\n               Accuracy : 1          \r\n                 95% CI : (0.9998, 1)\r\n    No Information Rate : 0.2847     \r\n    P-Value [Acc > NIR] : < 2.2e-16  \r\n                                     \r\n                  Kappa : 1          \r\n Mcnemar's Test P-Value : NA         \r\n\r\nStatistics by Class:\r\n\r\n                     Class: A Class: B Class: C Class: D Class: E\r\nSensitivity            1.0000   1.0000   1.0000   1.0000   1.0000\r\nSpecificity            1.0000   1.0000   1.0000   1.0000   1.0000\r\nPos Pred Value         1.0000   1.0000   1.0000   1.0000   1.0000\r\nNeg Pred Value         1.0000   1.0000   1.0000   1.0000   1.0000\r\nPrevalence             0.2847   0.1935   0.1744   0.1638   0.1836\r\nDetection Rate         0.2847   0.1935   0.1744   0.1638   0.1836\r\nDetection Prevalence   0.2847   0.1935   0.1744   0.1638   0.1836\r\nBalanced Accuracy      1.0000   1.0000   1.0000   1.0000   1.0000\r\n\r\n\r\nFigure 4 – Overall Statistics of randomForest Algorithm on Validation Data\r\n\r\nOverall Statistics\r\n                                     \r\n               Accuracy : 1          \r\n                 95% CI : (0.9903, 1)\r\n    No Information Rate : 0.2831     \r\n    P-Value [Acc > NIR] : < 2.2e-16  \r\n                                     \r\n                  Kappa : 1          \r\n Mcnemar's Test P-Value : NA         \r\n\r\nStatistics by Class:\r\n\r\n                     Class: A Class: B Class: C Class: D Class: E\r\nSensitivity            1.0000   1.0000   1.0000   1.0000   1.0000\r\nSpecificity            1.0000   1.0000   1.0000   1.0000   1.0000\r\nPos Pred Value         1.0000   1.0000   1.0000   1.0000   1.0000\r\nNeg Pred Value         1.0000   1.0000   1.0000   1.0000   1.0000\r\nPrevalence             0.2831   0.1878   0.1799   0.1481   0.2011\r\nDetection Rate         0.2831   0.1878   0.1799   0.1481   0.2011\r\nDetection Prevalence   0.2831   0.1878   0.1799   0.1481   0.2011\r\nBalanced Accuracy      1.0000   1.0000   1.0000   1.0000   1.0000\r\n\r\n\r\nGBM Model Results\r\nLike the rf method within the caret package, the gbm method within caret was also not able to resolve to a model.\r\n\r\nStep 6: Model Selection\r\nGiven the results of the three models, the random forest model has been selected as the best approach to predict the classe variable in the test data set. The 100% accuracy of the random forest model on the validation data set suggests an accuracy of 100% when the model is applied to the test data, and an error rate of 0%.\r\n\r\nStep 7: Apply Model to Test Data\r\nWhen the random forest model is applied to the 20 test data observations, the model predicts the classe variable for all 20 observations accurately.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}